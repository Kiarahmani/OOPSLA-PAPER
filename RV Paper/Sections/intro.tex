\section{Introduction}
\label{sec:intro}
Historically,
the \emph{de facto} system abstraction for developing distributed
programs has always included the 
ACID\footnote{Atomicity, Consistency, Isolation and
Durability} properties.
These properties, guarantee replication transparency  (i.e. requiring distributed
systems to \emph{appear} as a single compute and storage server to
users), and have resulted in development of standardized implementation and
reasoning techniques around \emph{strongly consistent} (SC) distributed
stores.
Although strong notions of consistency, are ideal for developement and
reasoning about distributed applications, 
they require extensive synchronization overhead which is unacceptable
for web-scale applications that wish to be ``always-on'' despite network
partitioning. Applications are therefore usually designed to tolerate
certain \emph{inconsistencies}, in exchange for availability and
low-latency. 
An extreme example is \emph{eventual consistency} (EC), where
the local state of each node at all time,
only represents an \emph{unspecified order} of an \emph{unspecified
subset} of the set of all updates submitted to the system globally.
Applications that  cannot tolerate the anomalous behaviors allowed under
EC, may choose to use various stronger instantiations, that are
collectively refered to as \emph{weak consistency} guarantees. 
Unfotunately, weak notions of consistency, are closely tied to  
specific data-store implementations, and in very few cases, such as 
\emph{causal consistency} (CC) for which there exists
relatively standard definitions and known implementation techniques,
users are usually offered with unnecessary levels of consistency and potental
performance loss\footnote{In fact, CC is the
strongest consistency guarantee that remains available under network
partitioning}. In order to face this problem, developers are forced to
inject their code with \emph{ad-hoc} anomaly tolerance mechanisms that are closely
tied to the application logic and conflate it with concerns orthogonal
to its semantics.
To illustrate this problem, we will present an example in section
\ref{sec:motiv}, where we introduce a simple distributed application
developed on top of an off-the-shelf eventually consistent data-store
(ECDS), and explain how it must be re-engineered from the scratch in
order to preempt certain undesired behaviors (i.e. enforce fine-grained
weak consistency requirements). As we will see, the ad-hoc
nature of such mechanisms confounds standardization, and complicates
reasoning, maintainability and reusability of the applications.

%
%

In this paper, we propose an alternative to the aforementioned
approaches that overcomes their weaknesses.  \tool is a lightweight
runtime system for Haskell that allows application developers to take
advantage of weak consistency without having to re-engineer their code
to accommodate anomaly preemption mechanisms.  The key insight that
drives \tool's design is that the hardness of reasoning about the
integrity of a distributed application stems from conflating
application logic with the consistency enforcement logic, and reasoning
about both \emph{operationally}.  By separating application semantics
from consistency enforcement semantics, admitting operational
reasoning for the former, and declarative reasoning for the latter
programmers are liberated from having to worry about implementation details of
anomaly preemption mechanisms, and instead focus on reasoning about
application semantics, under the assumption that specified consistency
requirements are automatically enforced by the data store at runtime.  
%
Our approach admits declarative reasoning for consistency enforcement via
a specification language that allows programmers formally specify the
consistency requirements of their application. 
The design of our
specification language is based on the observation that all
anomalous behaviors allowed under EC, occur as 
the result of nodes executing operations, before a certain set of
\emph{dependencies} arrive at that node. 
Users in \tool, can
specify arbitary dependency relations between updates, and the 
runtime system working on top of each ECDS replica, guarantees that an 
operation will only proceed if it can witness all of its dependencies. 
For example, \emph{lost-updates}, which is a very well known anomaly
under EC, occures when an operation from a session
is routed to a replica different than the replica that served the earlier
operaitons of the same session (because of transient system
properties, such as load balancing or network partitions),
and is successfully executed without witnessing the update from those
earlier  operations.
In this case, the dependency of the operations can be defined as the
updates from \emph{all previous operations from the same sessions}, and
\tool is guaranteed to temporarily block operations untill all such
dependencies become available at a replica.

%
%

To summarize the contributions of this paper:
\begin{enumerate*}[label=(\roman*)]  
\item We propose a specification language to
    express the fine-grained consistency requirements of applications 
    in terms of the dependencies between operations.

\item We describe a generic consistency enforcement runtime that
    analyzes each operation's consistency specification, and ensures
    that its dependencies are available before it is executed.    
    We formalize the operational semantics of the runtime, and prove its
    correctness and optimality (including \emph{minimum blocking} and
    \emph {minimum staleness}) guarantees. 
    
  \item We describe an implementation of our specification language and
    consistency enforcement runtime in a tool called \tool, which
    works on top of an off-the-shelf EC data store. We evaluate \tool
    over realistic applications and microbenchmarks, and present
    results demonstrating the performance benefits of making
    fine-grained distinctions between consistency guarantees, and the
    ease of doing so via our specification language.

\end{enumerate*}

The remainder of the paper is organized as follows. A system model that
describes the key notions of consistency and
replication is presented in Sec.~\ref{sec:sys_model}.  In
Sec.~\ref{sec:motiv} we provide a detailed example to further motivate
the problem.  In Sec.~\ref{sec:ctrt_language} and
Sec.~\ref{sec:semantics}, we formally present our specification language
and the high level operational semantics of the runtime system, with
correctness and optimality theorems. Sec.~\ref{sec:alg}
elaborates on the algorithmic aspects of our runtime that is key to
its efficient realization. Sec.~\ref{sec:eval} describes
implementation of \tool, and evaluates its applicability and practical utility.
Related works and conclusion are presented in
Sec.~\ref{sec:rel_works} and Sec.~\ref{sec:conclusion}








%%
%================================================
% OOPSLA VERSION
%================================================
\begin{comment}
Modern web-based applications are typically implemented as multiple
agents simultaneously serving clients, operating over shared data
objects replicated across geographically distributed machines.
Historically, replication transparency (i.e. requiring distributed
systems to \emph{appear} as a single compute and storage server to
users), has been the \emph{de facto} system abstraction used to
program in these environments.  This abstraction has resulted in the
development of standardized implementation and reasoning techniques
around \emph{strongly consistent} (SC) distributed stores.  Although
strong notions of consistency, such as \emph{linearizability} and
\emph{serializability}, are ideal to develop and reasoning about
distributed applications, they come at the price of availability and
low-latency.  Extensive synchronization overhead often necessary to
realize strong consistency is unacceptable for web-scale applications
that wish to be ``always-on'' despite network partitions.  Such
applications are therefore usually designed to tolerate certain
inconsistencies, allowing them to adopt weaker notions of consistency
that impose less synchronization overhead. An extreme example is
\emph{eventual consistency} (EC), where the application responds to
user requests using just the local state of the server to which the
client connects; this state is \emph{some} subset of the global state
(i.e., it includes an unspecified subset of writes submitted to the
application in an unspecified order).  Applications that may not
tolerate the level of inconsistency imposed by EC strengthen it as
needed, resulting in various instantiations that are stronger than EC,
but weaker than SC. The term \emph{weak consistency} is a catch-all
term used to refer to such application-specific weak consistency
guarantees.

Unfortunately, the \emph{ad hoc} nature of weak consistency confounds
standardization, with different implementations defining different
mechanisms for achieving weakly consistent behavior.  Oftentimes,
implementations are closely tied to application logic, complicating
maintainability and reuse.  To illustrate, consider a web application
that stores user passwords (encrypted or otherwise) in an
off-the-shelf EC data store (e.g., Cassandra~\cite{cassandra}). The
application allows an authenticated user to change her password,
following which the current authentication expires, and the user is
required to re-login.  Now, consider the scenario shown in
Fig.~\ref{fig:rmw-anomaly} where Alice changes her password, and
subsequently tries to login with the new password. This involves a
write of a new password to the store, followed by a read during
authentication.  However, because of transient system properties
(e.g., load balancing, or network partitions), Alice's write and the
read could be served by different replicas of the store, say $R_1$ and
$R_2$ (resp.), where $R_2$ may not (yet) contain the latest writes
from $R_1$. Consequently, Alice login attempt fails, even though she
types the correct password.

To preempt the scenario described above, applications might want to
enforce a stronger consistency guarantee that ensures reads from a
client session witness previous writes from the same session. The
consistency guarantee, known as \emph{Read-My-Writes/Read-Your-Writes}
(RMW/RYW), is one of several well-understood session
guarantees~\cite{terry-pdis94}, yet the methods used for its
enforcement are often store -and application- dependent. For instance,
Oracle's replicated implementation of Berkeley DB suggests application
developers implement RMW by querying various metadata associated with
writes~\cite{oracle-ryw}.  Each successful write to the store returns
a commit token, which is then passed with the subsequent reads to help
the store identify the last write preceding the read. The read
succeeds only if the write is present at the replica serving the read,
failing which the application has to retry the read, preferably after
some delay.  Fig.~\ref{fig:rmw-oracle} illustrates this idea.

The RMW implementation described above already requires considerable
re-engineering of the application (to store and pass commit tokens for
each object accessed), and conflates application logic with concerns
orthogonal to its semantics. On stores that do not admit metadata
queries (e.g., Cassandra), the implementation is even more complicated
as we describe in Sec.~\ref{sec:motivation}. Moreover, applications
sometimes require different consistency guarantees for different
objects.  In such cases different enforcement mechanisms must be
developed, forcing developers to simultaneously reason about their
respective properties \emph{in conjunction with} the application
state. This is clearly an onerous task.  Other alternatives such as
forgoing application integrity, or resorting to strong consistency,
sacrifice correctness or availability, both unappealing options.
\end{comment}

