\section{Introduction}
\label{sec:intro}

Historically, the \emph{de facto} system abstraction for developing
distributed programs has typically included ACID\footnote{Atomicity,
Consistency, Isolation and Durability} properties.  These properties
guarantee replication transparency (i.e. requiring distributed systems
to \emph{appear} as a single compute and storage server to users), and
make it straightforward to develop standardized implementation and
reasoning techniques around \emph{strongly consistent} (SC)
distributed stores.  Although such strong notions of consistency
simplify reasoning, they also introduce extensive synchronization
overhead which is often unacceptable for web-scale applications that
need to be ``always-on'' even in the presence of network partitions.
Such applications are therefore usually designed to tolerate certain
inconsistencies, allowing them to adopt weaker notions of consistency
that impose less synchronization overhead. An extreme example is
\emph{eventual consistency} (EC), where the local state of each
application server only represents an \emph{unspecified order} of an
\emph{unspecified subset} of the aggregate collection of all updates
submitted to the system globally. Applications that may not tolerate
the level of inconsistency imposed by EC are often equipped with
\emph{ad hoc} mechanisms to enforce the required level of consistency.
Unfortunately, such enforcement mechanisms are closely tied to the
application logic, confounding standardization, while complicating
application reasoning, maintainability, and reusability.

% Applications are therefore usually designed to tolerate certain
% \emph{inconsistencies}, in exchange for availability and low latency.
% An extreme example is \emph{eventual consistency} (EC), where the
% local state of each node only represents an \emph{unspecified order}
% of an \emph{unspecified subset} of the aggregate collection of all
% updates submitted to the system globally.  Consequently,
% application-level invariants that hold under strong consistency might
% break under eventual consistency.  Applications that cannot tolerate
% these anomalous behaviors under EC may choose to run under a stronger,
% albeit still weaker than SC, consistency model.  Weak notions of
% consistency, however, are closely tied to specific data-store
% implementations. More often than not, users are offered a
% choice of consistency levels that negatively impacts performance.  In
% order to overcome this issue, developers are forced to inject their
% code with \emph{ad-hoc} anomaly tolerance mechanisms that are closely
% tied to application logic, confounding standardization, and
% complicates application reasoning, maintainability, and reusability.

In this paper, we propose an alternative approach to weak consistency
enforcement that circumvents the aforementioned issues.  \tool is a
lightweight runtime verification system for Haskell that allows
application developers to take advantage of weak consistency without
having to re-engineer their code to accommodate anomaly preemption
mechanisms.  The key insight that drives \tool's design is that the
hardness of reasoning about the integrity of a distributed application
stems from conflating application logic with consistency enforcement
logic, requiring reasoning about both \emph{operationally}.  By
separating application semantics from consistency enforcement
semantics, however, admitting operational reasoning for the former,
and declarative reasoning for the latter, programmers are liberated
from having to worry about implementation details of anomaly
preemption mechanisms, allowing them to be focused instead on
application semantics, under the assumption that specified consistency
requirements are automatically enforced by the data store at runtime.
%
Our approach admits declarative reasoning for consistency enforcement
via a specification language that allows programmers to formally
specify consistency requirements.  The design of our specification
language is based on the observation that all anomalous behaviors
allowed under EC occur as the result of nodes executing operations
before certain \emph{dependencies} are satisfied.  Using \tool, one
can specify arbitary dependency relations between updates, and a
runtime monitoring and verification system working on top of each EC
data store replica guarantees that an operation will only proceed if
it can witness all of its dependencies.  For example,
\emph{lost-updates} is a well-known anomaly possible under EC that
occurs when an operation $o$ from a client session is routed to a
replica different than the replica that served earlier operations from
the same session, because of transient system properties, such as load
balancing or network partitions.  When this happens, $o$ may
successfully execute without witnessing updates from those earlier
operations. In this case, because $o$ is dependent on updates from
\emph{all previous operations from the same sessions}, \tool
guarantees to temporarily block operations until these dependencies
become available at $o$'s replica.  Of course, if this anamolous
behavior can be tolerated by the application, this level of dependency
tracking will be elided.

%
%

We make the following contributions:
\begin{enumerate*}[label=(\roman*)]  
\item We propose an expressive specification language to express
  fine-grained consistency requirements of applications in terms of
  the dependencies between operations.

\item We describe a generic runtime consistency enforcement mechanism
  that analyzes each operation's consistency specification, and
  ensures that its dependencies are available at the replicas on which
  it executes.  We formalize the system's operational semantics, and
  prove its correctness and optimality.
    
  \item We describe an implementation of these ideas in a tool called
    \tool, which works on top of an off-the-shelf EC data store.
    Evaluation over realistic applications and microbenchmarks,
    demonstrate the performance benefits of making fine-grained
    distinctions between consistency guarantees, and the ease of doing
    so via our specification language.

\end{enumerate*}

The remainder of the paper is organized as follows.  A system model
that describes the key notions of consistency and replication is
presented in Sec.~\ref{sec:sys_model}.  In Sec.~\ref{sec:motiv}, we
provide a detailed example to further motivate the problem.  In
Sec.~\ref{sec:ctrt_language} and Sec.~\ref{sec:semantics}, we formally
present our specification language and the high level operational
semantics of the runtime system, with correctness and optimality
theorems.  Sec.~\ref{sec:alg} elaborates on the algorithmic aspects of
our runtime that is key to its efficient
realization. Sec.~\ref{sec:eval} describes implementation of \tool,
and evaluates its applicability and practical utility.  Related work
and conclusion are presented in Sec.~\ref{sec:rel_works}.








%%
%================================================
% OOPSLA VERSION
%================================================
\begin{comment}
Modern web-based applications are typically implemented as multiple
agents simultaneously serving clients, operating over shared data
objects replicated across geographically distributed machines.
Historically, replication transparency (i.e. requiring distributed
systems to \emph{appear} as a single compute and storage server to
users), has been the \emph{de facto} system abstraction used to
program in these environments.  This abstraction has resulted in the
development of standardized implementation and reasoning techniques
around \emph{strongly consistent} (SC) distributed stores.  Although
strong notions of consistency, such as \emph{linearizability} and
\emph{serializability}, are ideal to develop and reasoning about
distributed applications, they come at the price of availability and
low-latency.  Extensive synchronization overhead often necessary to
realize strong consistency is unacceptable for web-scale applications
that wish to be ``always-on'' despite network partitions.  Such
applications are therefore usually designed to tolerate certain
inconsistencies, allowing them to adopt weaker notions of consistency
that impose less synchronization overhead. An extreme example is
\emph{eventual consistency} (EC), where the application responds to
user requests using just the local state of the server to which the
client connects; this state is \emph{some} subset of the global state
(i.e., it includes an unspecified subset of writes submitted to the
application in an unspecified order).  Applications that may not
tolerate the level of inconsistency imposed by EC strengthen it as
needed, resulting in various instantiations that are stronger than EC,
but weaker than SC. The term \emph{weak consistency} is a catch-all
term used to refer to such application-specific weak consistency
guarantees.

Unfortunately, the \emph{ad hoc} nature of weak consistency confounds
standardization, with different implementations defining different
mechanisms for achieving weakly consistent behavior.  Oftentimes,
implementations are closely tied to application logic, complicating
maintainability and reuse.  To illustrate, consider a web application
that stores user passwords (encrypted or otherwise) in an
off-the-shelf EC data store (e.g., Cassandra~\cite{cassandra}). The
application allows an authenticated user to change her password,
following which the current authentication expires, and the user is
required to re-login.  Now, consider the scenario shown in
Fig.~\ref{fig:rmw-anomaly} where Alice changes her password, and
subsequently tries to login with the new password. This involves a
write of a new password to the store, followed by a read during
authentication.  However, because of transient system properties
(e.g., load balancing, or network partitions), Alice's write and the
read could be served by different replicas of the store, say $R_1$ and
$R_2$ (resp.), where $R_2$ may not (yet) contain the latest writes
from $R_1$. Consequently, Alice login attempt fails, even though she
types the correct password.

To preempt the scenario described above, applications might want to
enforce a stronger consistency guarantee that ensures reads from a
client session witness previous writes from the same session. The
consistency guarantee, known as \emph{Read-My-Writes/Read-Your-Writes}
(RMW/RYW), is one of several well-understood session
guarantees~\cite{terry-pdis94}, yet the methods used for its
enforcement are often store -and application- dependent. For instance,
Oracle's replicated implementation of Berkeley DB suggests application
developers implement RMW by querying various metadata associated with
writes~\cite{oracle-ryw}.  Each successful write to the store returns
a commit token, which is then passed with the subsequent reads to help
the store identify the last write preceding the read. The read
succeeds only if the write is present at the replica serving the read,
failing which the application has to retry the read, preferably after
some delay.  Fig.~\ref{fig:rmw-oracle} illustrates this idea.

The RMW implementation described above already requires considerable
re-engineering of the application (to store and pass commit tokens for
each object accessed), and conflates application logic with concerns
orthogonal to its semantics. On stores that do not admit metadata
queries (e.g., Cassandra), the implementation is even more complicated
as we describe in Sec.~\ref{sec:motivation}. Moreover, applications
sometimes require different consistency guarantees for different
objects.  In such cases different enforcement mechanisms must be
developed, forcing developers to simultaneously reason about their
respective properties \emph{in conjunction with} the application
state. This is clearly an onerous task.  Other alternatives such as
forgoing application integrity, or resorting to strong consistency,
sacrifice correctness or availability, both unappealing options.
\end{comment}

