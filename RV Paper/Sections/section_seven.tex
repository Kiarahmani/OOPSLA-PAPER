\newpage
\section{Evaluation}
\label{sec:eval}
%
\input{Figures/fig_dist_table}
%intro: benchmark programs
%%%SJ: you need to introduce the implementation - what is
%%%the implementation written in, what does it run on top of,
%%%experimental setup, etc.
In this section, we present an evaluation study of our implementation,
including a report on
benchmark applications that utilize fine-grained weak consistency
requirements, expressable
in \tool's specification language.
Fig.\ref{fig:dist_table} presents seven such programs, that include
library definitions of individual replicated data types as well as larger programs consisting of multiple
replicated types. 

%multiple consistency levels for each program
%%%SJ: This is a very confusing paragraph - what is CC, what does
%%%length 1 or 2 mean in this context?
Each program offers various operations, each of which is assigned a
potentially different consistency requirement,
representing the need for a multi-consistnet environement for
efficient execution of the programs. Surprisingly, we found no program
intrinsically requiring causal consistency; all known consistency anomalies that operations
may be involved in are expressable with simple fine-grained contracts
composed of
dependency relations of length 1 or 2,
which differs from what was knwon in the context before, where all such
operations were considered to require CC.

%conjunction of consistency requirements for even a single operation
%%%SJ: quantify ``vast''
Additionally, in many cases we found operations that may be involved in
multiple anomalies, requiring simultaneous enforcement of different
consistency guarantees, which shows the infeasability of hand-writing
such guarantees, considering the vast set of known consistency
anomalies. 
%
%example
For example, consider a bank account application, which offers
\dRV{}, \wdRV{} and \gbRV{} operations, where \wdRV{} is a
strongly consistent operation that succeeds only if there are sufficient
funds in the account. There are two anomalous scenarios associated with
\gbRV{} in this program:
\begin{enumerate*}[label=(\roman*)]
\item when a user performs a \dRV{} that is not reflected
in subsequent \gbRV{} operations;
\item when a \gbRV{} witnesses a \wdRV{} effect without witnessing all
\dRV{} effects visible to it,  resulting in \gbRV{} returning a
potentially (incorrect) negative balance.
\end{enumerate*}
As presented in Fig.~\ref{fig:dist_table}, to preempt
these anomalies, \gbRV{} requires
both \rmwCTRT{} and \visCTRT{} guarantees to be simultaneously satisfied.
\input{Figures/fig_comparison}

% performance evaluation
We have deployed \tool on a cloud cluster,
consisting of three fully replicated Cassandra replicas, running on
seperate machines within the same
datacenter. 
Each machine is instantiated with a
\tool shim layer, that responds to clients,  
 which are instantiated on a virtual machine 
co-located with one of the replicas on a machine.
We deploy the cluster on three \texttt{m4.4xlarge} Amazon EC2 instances
in the US-West (Oregon) region, with an inter-machine communication time of 5ms.

% The problem with Cassandra
%%%SJ: is there any references you can cite to indicate this
%%%decision is realistic, or consistent with other papers in this space?
Inter-replica communications in Cassandra uses TCP connections,
causing all messages to get delivered with no loss and reordering,
which is in practice, far more consistent than EC, and masks out the
performance gain from our fine-grained consistency guarantees.
Consequently, to simulate a realistic EC environment, we inject
artificial message loss at the shim layers, where message delivery is
delayed for 1 second in case it is lost.

%The latency and staleness gain using fine-grained consistency
Fig.~\ref{fig:eval}(a) and \ref{fig:eval}(b) represent
our experimental results, with a workload generated 
by 50 concurrent clients repeatedly running sessions, each composed of three
operations, where operations uniformly choose from 5 objects,
performed under a specified consistency level. 
We increase the
percentage of delayed messages from 0 to 14.  Each experiment ran for
100 repeaded sessions per client. In addition to client perceived
latency, we also measure the staleness of operations, which we define as
the average ratio of the number of visible effects,
to the number of all available effects, at the time an operation is executed.

% latency result
In the first set of experiments, we measure latency under
three different \LB{} contracts, all implemented in \tool. As
expected,
causal consistency and RMW experience respectively the highest and the
lowest
performance loss as the percentage of lost messages is increased\footnote{In fact, 
they define the strongest and the weakest
\LB{} dependency relations expressable in our language:
$(\xrightarrow{\soZ})$ and $(\xrightarrow{(\soZ\cup\visZ)^*})$}.
With only a 4\% percent message loss rate, we see $17\%$ higher latency under an MR
contract compared to RMW, and similarly 67\% higher latency in CC
compared to MR; with $10$ percent message loss, the numbers are
increased to $18\%$ and $87\%$.


%latency result
Similarly, we repeated the experiment with 3 \UB{} contracts. Here, a
\emph{causal visibility} (CV) contract (i.e. {\footnotesize $ \forall a.
a\xrightarrow{(\soZ\cup\visZ)^*;\visZ}\hat{\eta}\Rightarrow a\xrightarrow{\visZ}
\hat{\eta} $}), yields the most stale data when the percentage of lost
messages is increased, whereas staleness in MW is the lowest and is
barely effected. We report $3\%$ ($6\%$) difference 
between staleness of data under MW and 2VIS, and $4\%$ ($7\%$)
difference between 2VIS and CV,
at four (ten) percante message
loss rate.



%handwritten compared
Finally, in order to provide evidence on the practicality of \tool, we
implemented an ad-hoc mechanism to prevent the lost-updates anomaly,
for a simple counter application. Fig.~\ref{fig:eval}(c) shows the
latency results of this application compared to the same in \tool,
under the same setting as before (albeit with no message loss).  We
see $78\%$ higher latency for the handwritten code compared to \tool
with 50 concurrent clients.  Beyond the performance numbers, we
experienced many bookkeeping complications with the handwritten
implementation, mainly because of the lack of meta-data queries in
Cassandra which needs strongly consistent table alterations at the
beginning and the end of each session.










































