\section{Implementation}
\label{sec:alg}
\tool is implemented as an extension to a GHC Haskell add-on called
Quelea \cite{quelea}.  Quelea maintains a causally
consistent~\cite{bolton} cache on top of Cassandra, and \emph{all}
operations whose contract is satisfied under causal consistency, are
performed witnessing that cache (even if they require weaker
guarantees.).

In \tool, we maintain a generic cache in which operations maintained
by the cache are associated with tags, and are allowed to witnesses
only the subset of effects in the cache that also have that tag
(i.e. effects that are in the \emph{logical cache} associated with
that operation). We implemented a dependency finder mechanism in
\tool, that is used to verify the presence of arbitrarily defined
dependencies of an effect in each logical cache. Consequently, \tool's
filtration and blocking mechanisms are added to the runtime system,
which rely on this dependency finder to keep each logical cache
consistent according to its associated contract. Specifically, given a
dependency relation $R$ and a replica containing a localset $V$ of
effects, an effect $\eta$ is allowed to enter a logical cache only if
$\trunc{R}_V^{-1}(\eta)$ is already in that cache.


%the memoization technique
Considering the arbitrary length of the dependency relations that may
generated and the fact that verifying the presence of dependencies for
an effect might fail for an unbounded number of trials until all
dependencies arrive, special care must be taken to ensure performance
does not grade at scale.  We implemented a number of techniques to
improve cache efficiency such as memoization that extends the binary
notion of dependency presence to the \emph{degree of dependency
  presence} (\DDP{}) representing the maximum \emph{depth} (or size)
of the dependencies of an effect, whose presence has already been
verified.  Consequently, when verification fails, we can avoid
checking previously computed and known dependencies when subsequent
effects arrive.  \tool's runtime, by performing periodic \DDP{}
refreshes, tries to assign larger \DDP{} values to each effect when
more dependencies arrive at the replica. We leave the details of this
technique, captured as an operational semantics in appendix
\ref{appendix:large_semantics} for the interested reader.








\begin{comment}
%intro
While the semantics defines \emph{what} \emph{what} subset of
effects at a replica  must be witnessed by every operation, it does
not address \emph{how} to realize efficient construction of that subset.

%how cache works
\tool maintains a consistent cache on top of each replica by periodically
reading from the underlying ECDS, where an effect $\eta$ is moved to the
cache only if the cache already includes {\footnotesize
$\trunc{R}_V^{-1}(\eta)$}. Consequently, all operations under \UB{}
contracts can be immediately executed
by witnessing the cache, which is a closed subset (not necessarily maximal) 
of $V$, the set of effects present at the replica. 
Moreover, \LB{} contracts can also be satisfied, by blocking  operations 
until effects of all previous operations from the same session enter
the cache, in which case the current operation can proceed and witness \emph{all}
effects present at the replica.

%the memoization technique
Additionally, we implemented a simple memoization technique in \tool\
that extends the binary notion of dependency presence to the
\emph{degree of dependency presence} (\DDP{}) representing 
the maximum \emph{depth} (or size) of the dependencies of an effect, whose presence have been
verified so far. 
Consequently, when verification of the presence of
dependencies for an effect fails, the runtime system can avoid redundant
computations the time it tries to verify the same property for the
same effect.
\tool's runtime, by performing
periodic \DDP{} refreshes, tries to assign larger \DDP{} values to each effect
when more dependencies arrive at the replica. 
Specifically, at each refresh, the \DDP{} of an effect $\eff$ is increased from $i$ to $i+1$ if
all effects in $r_{i+1}^{-1}(\eff)$ already have \DDP{} values at least
equal to $i$.

% example
%%%SJ: This is confusing - you need to give an example of a
%%%redundant computation, otherwise this paragraph will not be understood by the reader
For example, consider a contract with dependency relation
$\Rel=\soZ;\visZ;\soZ$, and a newly arrived effect $\eta$ to the
replica, whose \DDP{} is initially set to 0. 
During the next refresh, $\eta$ is given the value 1, if all
effects in $\soZ^{-1}(\eta)$ have \DDP{} equal to 0 (i.e. are present at the
replica). Similarly, $\eta$ is given the value 2, if all effects in
$\visZ^{-1}(\eta)$ have \DDP{} value of at least 1, which means that 
$(\soZ;\visZ)^{-1}_V(\eta)$ is now present at the replica and
consequently, $\eta$ can
be safely added to the consistent cache (Fig.\ref{fig:avail_deg}).
Using this technique, \tool avoid redundant computations of potentially large
dependency relations.
%\input{Figures/fig_availability_degrees.tex}
\end{comment}
